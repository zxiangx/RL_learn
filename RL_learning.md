# 强化学习笔记（根据：动手学习强化学习）

[toc]

​		强化学习的知识点初学时略有复杂，理解内容较多，因此开始整理学习笔记，总结所学知识，加深理解。
​		以下内容为对各种算法进行总结，找出算法核心要点，深刻理解伪代码，最终目的为熟练将代码复现，并且之后可以轻松写出动作离散和连续的情况。

​		总结的方向主要是：解读伪代码，从伪代码中理解算法的原理，逐句分析伪代码的意义。分析伪代码的过程与看代码并加以注释相结合。

## 马尔可夫决策过程

马尔可夫过程具有性质：每一个状态都只和上一个状态有关，换句话说下一个状态只取决于当前的状态。

### 各个参数的意义

markov决策过程一共有5个参数：<$\boldsymbol{S,A,P,r,\gamma}$>，其中：

$\boldsymbol S$: 状态集合，包含了整个过程可能经历的所有状态。

$\boldsymbol A$:动作集合，包含了整个过程中在所有状态可能作出的全部动作。

$\boldsymbol P$:状态转移函数，$\boldsymbol P(s^{'}|s,a)$表示在状态$s$下执行动作$a$后，会进入到状态$s^{'}$的概率，为一个二维矩阵。

$\boldsymbol r$:奖励函数，$\boldsymbol r(s,a)$表示在状态$s$下执行动作$a$的**这步操作**可以获得的奖励。

$\boldsymbol \gamma$:折扣因子，为了让智能体在短时间内到达终点，对远处的奖励进行折扣处理。

除此之外，需要习得的最为重要的东西就是：策略$\boldsymbol \pi$，这个策略会指导智能体在相应的状态下执行一个特定的动作，而学习的过程就是不断更新策略的过程，使得其可以指导智能体执行可以获得最大奖励的动作。策略$\boldsymbol \pi$可以表示为一个概率：
$$
\boldsymbol \pi(a|s) = \boldsymbol P(A_t=a|S_t=s)
$$
也即在状态$s$下执行动作a的概率。正常来说，在一个状态下执行任何一个动作都是可以的，而这里的策略对作出的动作赋以概率，使得智能体在不同的状态下对动作的执行有所偏好。

现在的问题是如何更新策略$\boldsymbol \pi$。想要把已知的参数和策略结合起来，采取的方法是设定两个函数：

$\boldsymbol V$: $\boldsymbol V^\pi(s)$表示在策略$\pi$下，从状态$s$出发可以获得的期望奖励。

$\boldsymbol Q$: $\boldsymbol Q^\pi(s,a)$表示在策略$\pi$下，在状态$s$执行了动作$a$之后可以获得的期望奖励。

学习策略$\pi$的过程，其实就是学习函数$V,Q$的过程。习得了这两个函数，就可以利用其获得最优的策略。

### 贝尔曼期望方程

从$V,Q$的定义可以看出，$V,Q$之间满足着某些定量关系：
$$
V^\pi(s)=\sum_{a\in A} \pi(a|s)Q^\pi(s,a)\\
Q^\pi(s,a)=r(s,a)+\gamma\sum_{s^{'}\in S}P(s^{'}|s,a)V^\pi(s^{'})
$$
利用这两个公式，可以推导出$V,Q$两者自身的递推公式，也即**贝尔曼期望方程**：
$$
V^\pi(s)=\mathbb{E}_\pi[R_t+\gamma V^\pi(S_{t+1})|S_t=s]
$$
这里条件概率的条件只有一个$S_t$，表示当前状态（第$t$个状态）位于$s$。$R_t$表示第$t$步动作获得的奖励，$S_t=s$表示第$t$个状态为$s$，$V^\pi(S_{t+1})$表示从第$t+1$个状态出发可以获得的期望奖励，这里$t+1$个状态是未知的，所以要与状态转移函数进行概率加权。
$$
Q^\pi(s,a)=r(s,a)+\mathbb E_\pi[\gamma Q^\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a]
$$
这里条件概率的条件有两个，$S_t,A_t$，表示第t个状态位于$S_t$，而且已经在这个状态执行了动作$A_t$。

## 算法1：蒙特卡洛方法

蒙特卡洛方法，说得简单一点就是取样求平均值。现在的主要目的就是更新$V,Q$两个函数，但如果使用了蒙特卡洛方法的话就只需要关注函数$V$即可。现在，就是要取得大量的$V$值，然后对同一个状态$s$的$V$值求取平均值，将其作为$V(s)$即可。

但是取这个$V$值还需要点技巧，**技巧就是**：

1. 收集大量的序列。

2. 对于每一条序列，将其中的每一个状态作为初始状态，计算其到序列末尾的所有奖励之和作为一个$V$值

通过这种方法，就可以获取大量的$V$值，然后可以对$V(s)$进行更新了。

但是更新也有点小技巧。如何计算状态到序列末尾的所有奖励和？（包括折扣因子）方法就是：从序列的尾部开始，依次往前走，把当前状态的$V$值乘上$\gamma$再加上前面状态到当前状态的奖励，就得到了前面那个状态的$V$值。
然后要如何用现在得到的$s$状态的一个$V$值去更新最终的$V(s)$？利用如下公式：
$$
N(s)=N(s)+1 \\
V(s)=V(s)+\frac{v-V(s)}{N(s)}
$$
其中$v$为当前计算出的状态$s$的$V$值。通过这种方式可以防止求和结果溢出，而且省去了最终相除的操作。

总结一下就是：**收集序列，从后往前，技巧更新**。

## 算法2：动态规划

### 策略迭代

策略迭代的主要步骤就2个：

1. 更新价值函数$V$。因为对于不同的策略$\pi$，会有不同的价值函数，所以先更新价值函数以契合当前的策略$\pi$。
2. 更新策略$\pi$。因为现在已经找到了$\pi$对应的价值函数$V$，所以遍历所有的状态$S$，选择最优的动作作为$\pi(s)$。如果有多个动作的价值相等且都为最大值，那么就把1均分成多分平均分给多个动作（如果策略发生改变就回到第一步）。

那么现在的主要问题是如何更新价值函数，这里用到了一点**动态规划思想**和一点**十分模糊的全局思想**：

要更新当前状态的$V$值，可以通过后面一个状态的$V$值来确定。有了后面的$V$值，按照概率加权并加上奖励即可得到当前状态的$V$值，所以这里用到了一点点动态规划思想。
但问题是后面的状态不一定已经更新好了。更何况，对于悬崖这样的环境，后面的状态也可以进入到当前的状态，dfs获得$V$值比较困难。这时候就可以使用逼近的方式，也即对于每一个状态都直接用后面状态的$V$值来计算得到，不管后面状态的$V$值是否已经是最优结果。不断执行对所有状态$S$的价值更新，直到某一次发现所有的$V$值都无法被更新了为止（即更新前后差距都特别小）。这就是一种**十分模糊的全局思想**。

### 价值迭代

价值迭代比策略迭代简单很多，只有策略迭代的第一步，但是有些区别。

策略迭代的价值函数更新是利用后面的$V$值和概率加权后加上奖励得到当前的$V$值，然后不断迭代，最终让$V$值收敛至$V^\pi$。而这里的区别就是，不是单纯的得到策略迭代中的$V$值，而是遍历所有的动作$a$，找到最大的$V$值。

策略迭代中$V$值的更新方法为：
$$
V(s)=\sum_{a \in A}\pi(a|s)\bigg(r(s,a)+\gamma\sum_{s^{'}\in S}P(s^{'}|s,a)V(s^{'})\bigg)
$$
更新结果：$V$收敛到$V^\pi$。

价值迭代中$V$值更新方法为：
$$
V(s)=\max_{a \in A}\{r(s,a)+\gamma\sum_{s^{'}\in S}P(s^{'}|s,a)V(s^{'})\}
$$
更新结果：直接得到了最优的$V$值，可以直接从中得到最优策略$\pi$。

## 算法3：时序差分

首先搞清楚什么是**无模型强化学习**。

这里无模型，指的是缺少两个量：奖励函数$r(s,a)$以及转移函数$P(s^{'}|s,a)$。在这种模型中，有两种最重要的函数：step和reset。

step：在当前状态下执行一个动作到下一个状态，step函数会返回3个信息：这个动作到达的下一个状态next_state, 这个动作获得的奖励reward，进行了这个动作之后是否到达了终点done。

reset：让智能体回到初始状态，用于进行下一个序列。

也就是说，你无法直接得到环境的奖励和转移函数信息，但是在环境中每走一步，环境都会告诉你这一步的所有有用信息。所以，这里的**无模型**指的是不知道模型的全部信息，奖励、转移概率这些信息需要不断去探索得到。

接下来说回时序差分。

1. **时序差分直接处理$Q(s,a)$**。对于一个无模型强化学习，缺乏了转移函数$P$，无法对$P(或\pi)$进行直接更新。如果只是对价值函数$V$进行了更新，知道了某个状态的$V(s)$，但是你往哪里走啊？不知道，最终还是按照环境的step瞎走。所以这里**时序差分**方法不对$V$进行更新，而是直接操作$Q(s,a)$，也即在某个状态执行某个动作可以得到的价值。这样做非常有效，因为习得了$Q$之后，可以通过寻找最优的动作来指导智能体运动，而不是盲目乱走。

2. **无模型环境下只能一步步探索**。类似先前的蒙特卡洛方法。区别在于：蒙特卡洛方法需要一直执行动作，直到走到了环境的终点才停止。而这里不一样，时序差分每一步的探索都马上利用起来，用来更新$Q(s,a)$。想要走多少条序列自己决定。

时序差分一共有两种方式：**sarsa和Q-learning**.

### sarsa

sarsa的更新方式为：
$$
Q(s_{t},a_{t})=Q(s_t,a_t)+\alpha(r+\gamma Q(s_{t+1},a_{t+1})-Q(s_t,a_t))
$$
本来根据蒙特卡洛方法，这里的$\alpha$应该是要用来平衡期望的，而这里的$\alpha$则是一个自由的超参数。

1. **动作选取策略: $\epsilon$-贪婪策略**。一开始，$Q(s,a)$是随机初始化的，然后根据step返回的信息进行更新。在状态$s_t$时，要执行什么动作$a_t$呢？正常来说，是要选择使得$Q(s)$值最大的动作，即$\arg\max_{a_t}(Q(s_t,a_t))$。但这样可能会出现一种情况：由于初始化非常不稳定的缘故，有一些$Q$值一开始就被初始化得特别小，所以这样的动作就始终被忽视了，而这样的动作很可能就是最终的最优解。所以，如果直接草率的选择目前看来最优的动作$a$不免有些短浅，所以选择动作的策略进行了优化，变为**$\epsilon$-贪婪策略**。也就是：
    给定一个概率$\epsilon$，使得在选取动作时，能有$\epsilon$的概率在所有的动作中选择一个执行，而不是当前最优的动作一个独大。

2. **在线策略**: 前后执行的选取策略相同。现在正处于状态$s_t$, 并且使用$\epsilon$-贪婪策略选择了一个动作$a_t$. 然后把动作传入函数step, 会返回3个参数,也就可以得到公式中的$r,s_{t+1}$. 但还有一个量: $a_{t+1}$没有得到, 这个量需要再从状态$s_{t+1}$出发选择一个状态. 对于在线策略sarsa, 这个动作的选取仍然遵循贪婪策略. 

### Q-learning

Q-learning和sarsa唯一的区别就在于: 第二步动作的选取方式. sarsa第二步同样采用贪婪策略进行选取, 而Q-learning则是采用原来被舍弃的方法: 选择当前最优的动作. 因此更新方法为:
$$
Q(s_{t},a_{t})=Q(s_t,a_t)+\alpha(r+\gamma \max_{a}Q(s_{t+1},a)-Q(s_t,a_t))
$$
还有一个dyna-Q方法, 和Q-learning的区别是: 构造了一个新的结构: 模型Model, 是一种字典, 用来存储先前序列用过的所有$(s,a)$->$(r,s^{'})$的状态转移情况。（**其实并不是全部的序对。**这里的Model是一个类似于Q网络的一个模型，所以这个Model就是一个从(s,a)到(r,s')的一个字典，当获得了一个(s,a)到(r,s')的序对，**会把之前的覆盖掉**）。上面所说的**无模型强化学习**的概念有些片面，其实如果使用与环境交互得到的结果来构建一个模型Model，那么这种后来构建了一个模型的方法也被称为**基于模型的强化学习**。可以看出，Q-learning就是一种基于模型的强化学习。实际上, Q-learning选取多个序列的目的就是获取很多这样的序对, 然后利用不断更新的模型$M$对$Q$进行更新. 所以, 构造这样一个模型, 就可以将之前所得到的一些序对拿来用**（其实就是上一次在状态s进行的动作a）**, 重复进行更新.   (利用状态$s$和动作$a$, 得到的奖励$r$和下一个状态$s^{'}$, 这个序对完全是由环境内置的函数step得到的, 所以无论是什么时候得到的都是一样的序对, 因此先前得到的序对和现在得到的序对都是可以很好利用的) 因此有了Model, 就可以在每次在序列中往前进的每一步都从$M$中选择n个序对进行n次$Q$值更新, 使得$Q$值快速收敛, 需要的序列数也减少了. 

从这里可以看出, 这里的离线策略确实像是将水接到脸盆中洗手, 而之前的在线策略则是直接选取很多个序列, 然后将序列中的每个状态转移情况用完就扔, 样本利用率很低; 而这里的离线策略将状态转移情况都存储在Model中, 大大提高了样本利用率. 

## 算法4：DQN

标签：无模型，离线策略算法，针对离散动作(因为有要选择最优的动作的步骤)

DQN，也即Deep Q Network，**深度Q网络**，顾名思义就是利用神经网络的方式来实现$Q$函数。这里DQN算法针对离散的动作，将连续的状态作为输入层（即输入层每一个结点都表示一个状态，输入的数据为连续的状态），加入一个隐层（增强网络的表达能力），输出层为在该状态下采取各个动作的价值（也即Q(s,a)）。

既然有神经网络，那么就一定有损失函数以及参数更新方法（梯度下降）。考虑到Q-learning的更新方法，TD目标为：
$$
r+\gamma \max_a Q(s_{t+1},a)
$$
于是将损失函数定义为均方差形式：
$$
\omega^* = \arg\min_\omega\frac{1}{2N}\sum^{N}_{i=1}\bigg[Q_\omega(s_i,a_i)-\bigg(r_i+\gamma\max_{a'}Q_\omega(s_i',a')\bigg)\bigg]^2
$$
这里的$N$指的是从**经验回放池**中取出的经验数量。每次更新都从回放池中取出一定量的经验过程并计算一遍损失函数，然后反向传播即可。经验回放池的优点有：

1. 提高样本利用率。
2. 防止非独立同分布的数据对网络产生较大影响。

除了回放池，DQN算法还有一个特别点：使用目标网络。目标网络，顾名思义，就是用来计算目标的网络。这里使用目标网络计算得到的目标就是式子中的TD目标：$r_i+\gamma\max_{a'}Q_\omega(s_i',a')$。这里用$Q_{w^-}$表示目标网络，则损失函数可以表示为：
$$
\omega^* = \arg\min_\omega\frac{1}{2N}\sum^{N}_{i=1}\bigg[Q_\omega(s_i,a_i)-\bigg(r_i+\gamma\max_{a'}Q_{\omega^-}(s_i',a')\bigg)\bigg]^2
$$
注意这里可以将$Q_\omega(s_i,a_i)$和$r_i+\gamma\max_{a'}Q_{\omega^-}(s_i',a')$用两个向量表示，然后直接用内置函数计算均方误差。

伪代码：

![DQN](RL_learn_png\DQN.png)

主要思想是走一步看一步，整理一下流程：

1. 走一步，结果加入池子

2. 从池子里抽一点数据出来

3. 计算TD目标张量（集合），均方误差更新网络

4. 看周期更新目标网络

   -> **以上步骤套入双循环**(即走到头一层，走n条序列一层)

### double_DQN与dueling_DQN

#### double_DQN

对于TD目标的计算，采用双网络的方式，而不是完全由目标网络决定。也即：
$$
a=\arg\max Q_\omega(s_{i+1},a) \\
y_i=r_i+\gamma Q_{\omega^-}(s_{i+1},a)
$$
即目标网络带入的动作$a$不是使其最大的动作，而是使得实时更新网络($Q$网络)最大的动作。因为如果带入的动作始终都是使得目标网络最大的动作，那么就会导致正向误差不断积累，最终引起正向过度估计，使用双网络的方式可以有效缓解这种误差积累。

代码区别：只有计算TD目标那一步和DQN代码有区别。

#### dueling_DQN

将价值函数$V$和优势函数$A$分开学习（$Q=V+A$），最后取：
$$
Q(s,a)=V(s)+A(s,a)-\frac{1}{|\boldsymbol A|}\sum_{a'}A(s,a')
$$
也即Q减去了一个优势函数的平均值，通过这种方式求解出$Q(s,a)$。将状态传入网络，会得到一维的价值函数值$V(s)$和$|\boldsymbol A|$维的张量$A(s)$，然后将二者相加再减去$A(s)$的平均值即可。这样做可以使得智能体对价值函数和动作进行针对的训练。

代码区别：只有设计网络层层结构的时候有区别。

## 算法5：策略梯度下降

### 说明：后面的3个算法也都是基于策略的算法，即AC，TRPO，PPO，基于策略的算法有一个共同点：都是先基于策略求出策略更新的方向，但这个方向中都会带有$\nabla_\theta$一项，也即可以直接把这个导函数还原，然后直接反向传播梯度下降即可对策略参数$\theta$更新。

标签：基于策略，无模型，在线策略算法，

可以看出上面的所有方法都围绕着一个东西进行讨论：**值**。以上策略要么是更新动作价值函数$Q(s,a)$，要么是更新状态价值函数$V(s)$。那么这里来讨论一个全新的思路：直接修改策略$\pi$.

这里把策略$\pi$参数化，令其为$\pi(\theta)$。事实上，对于$Q$网络而言，这里的参数$\theta$就是网络的全部参数，例如权重和偏置。那么不断优化策略$\pi$的过程，就是不断对参数$\theta$进行修改的过程。

将需要优化的目标定义为：
$$
J(\theta)=\mathbb E_{s_0}[V^{\pi_\theta}(s_0)]
$$
也即优化的目标是使得总体的状态价值函数达到最大。并且推导得到目标对参数$\theta$的梯度为：（推导省略）
$$
\nabla_\theta J(\theta) \propto \mathbb E_{\pi_\theta}[Q^{\pi_\theta}(s,a)\nabla_\theta\log\pi_\theta(a|s)]
$$
这个公式里面包含了一个$Q(s,a)$，所以可以使用蒙特卡洛方法来获取大量的$Q$值（为什么不可以使用经验回放池来获取大量的$Q$？因为**这里后面那个梯度是针对当时的策略进行求导，而策略又是不断更新的，**所以即使设置了经验回放池也用不了里面的数据，因此为在线策略）。获得了一条序列之后，可以得到参数$\theta$更新的方向：
$$
\theta=\theta+\alpha\sum^T_{t=1}\psi_t\nabla_\theta\log\pi_\theta(a_t|s_t)
$$
这里的$\psi_t$就是从序列中第$t$个状态往后走到头得到的奖励。具体实现可以是从后往前每次计算这个$\psi$，然后乘上$\pi_\theta(a_t|s_t)$，再反向传播就可以了，最后把$T$次反向传播累计得到的结果梯度下降一下就行了。损失函数设计成$-\sum^T_{t=1}\psi_t\log\pi_\theta(a_t|s_t)$即可，最后梯度下降直接会把后面那一坨更新到$\theta$后面。（这里的跨度有点大，主要理解：反向传播过程就是求导$\nabla_\theta$过程，梯度下降过程就是参数$\theta$更新过程）这里的$\alpha$就是学习率。

其实不论什么策略，最终都是要进行梯度下降，都是要更新网络参数，但是这里的策略梯度下降是直接把策略$\pi$作为研究目标，换了一个计算参数更新梯度的方法。

![策略梯度下降](RL_learn_png\策略梯度下降.png)

## 算法6：AC算法（Actor-Critic）<后面的算法都用到了AC>

### 说明：AC算法，TRPO，PPO都用到了AC双网络，并且A网络都是对于离散动作就输出动作的概率分布，对于连续动作就输出动作分布的均值和标准差；C网络都是输入状态，然后返回状态的价值$V(S)$。事实上，对于非确定性策略，AC网络都是这样。

标签：同上

AC算法和策略梯度下降差不多，区别主要就两个：

1. 设计了价值网络Critic，和Actor策略网络一起更新。

2. 这里的$\psi$不再是$Q$，而是TD残差：$r_t+\gamma V^{\pi_\theta}(s_{t+1}) -V^{\pi_\theta}(s_t)$。这里的V值需要利用价值网络Critic得到。

AC算法的优点：可以提高鲁棒性，减小方差。

AC算法比reinforce算法要省事一点，因为不需要计算一大堆的$Q$值，而只需要利用张量计算来简单得到TD残差张量，和log出的张量相乘，求一手平均值就可以得到梯度方向，并且兼顾序列中所有的$(s,a)$。以上是策略网络的更新，至于价值网络的更新，直接利用TD目标和实际$V$值计算均方误差，然后反向传播梯度下降就没了。

**AC算法的伪代码（非常关键）**：

![AC](RL_learn_png\AC.png)

可以看出，AC算法主要循环了以下几个步骤：

1. 获得序列
2. 计算TD残差张量（也可以说是TD残差集合）
3. **均方差**更新价值网络
4. **复杂梯度**更新策略网络

## 算法7：TRPO算法

标签：同上

TRPO在AC算法上进行的优化是：加入了一个信任区间，使得策略在小范围内进行更新，不至于步长过大掉入坑里。

直接给出TRPO算法的优化目标：
$$
\max_\theta\quad \mathbb E_{s\sim \nu^{\pi_{\theta_k}}}\mathbb E_{a\sim \pi_{\theta_k}(\cdot|s)}\Big[
\frac{\pi_\theta(a,s)}{\pi_{\theta_k}(a,s)}A^{\pi_{\theta_k}}(s,a)\Big] \\
\mathrm{s.t.} \quad\mathbb E_{s\sim \nu^{\pi_{\theta_k}}}[D_{KL}(\pi_\theta(\cdot|s),\pi_{\theta_k}(\cdot|s))]\le\delta
$$
具体推导过程就不说了。总之就是得到当前已经更新了k次的策略$\theta_k$之后，要如何找到下一个要变成的$\theta$？上面给出了答案，下面是条件，也即防止两个梯度之间相差太大，$D_{KL}$方法就是用于计算两个策略之间的距离，$\delta$是一个超参数。

但是直接求解这个东西没有头绪，于是用泰勒展开：
$$
\mathbb E_{s\sim \nu^{\pi_{\theta_k}}}\mathbb E_{a\sim \pi_{\theta_k}(\cdot|s)}\Big[
\frac{\pi_\theta(a,s)}{\pi_{\theta_k}(a,s)}A^{\pi_{\theta_k}}(s,a)\Big] \approx g^T(\theta-\theta_k)\\
\mathbb E_{s\sim \nu^{\pi_{\theta_k}}}[D_{KL}(\pi_\theta(\cdot|s),\pi_{\theta_k}(\cdot|s))] \approx
\frac{1}{2}(\theta-\theta_k)^TH(\theta-\theta_k)
$$
这里$g,H$分别是梯度和海森矩阵的意思。后面的太过复杂，用到了一大堆的海森矩阵处理，就不整理了，直接切入更简单的算法：ppo。

## 算法8：PPO算法

标签：同上

PPO算法相比于TRPO算法的优势就在于将条件融入到优化目标里面，将条件优化问题转化为无条件优化问题。PPO主要有两种：

1. ppo-惩罚。直接把距离$D_{KL}$放到优化目标里面，优化目标变成：
   $$
   \max_\theta\quad \mathbb E_{s\sim \nu^{\pi_{\theta_k}}}\Big[\mathbb E_{a\sim \pi_{\theta_k}(\cdot|s)}
   \frac{\pi_\theta(a,s)}{\pi_{\theta_k}(a,s)}A^{\pi_{\theta_k}}(s,a)-\beta D_{KL}(\pi_\theta(\cdot|s),\pi_{\theta_k}(\cdot|s))\Big]
   $$
   这里的系数$\beta$还和后面的$D_{KL}$有关系，这里也不多说，直接换到下一个方法。

2. PPO-截断。在重要性采用得到的系数上动手脚，优化为：
   $$
   \max_\theta\quad \mathbb E_{s\sim \nu^{\pi_{\theta_k}}}\mathbb E_{a\sim \pi_{\theta_k}(\cdot|s)}\bigg[\min\bigg(\frac{\pi_\theta(a,s)}{\pi_{\theta_k}(a,s)}A^{\pi_{\theta_k}}(s,a),\mathrm{clip}\Big(\frac{\pi_\theta(a,s)}{\pi_{\theta_k}(a,s)},1-\epsilon,1+\epsilon\Big)A^{\pi_{\theta_k}}(s,a)\bigg)\bigg]
   $$

PPO截断比PPO惩罚效果更好。

上面这个式子就是策略网络Actor的更新方向，即可以取负数然后传播下降；价值网络Critic的更新从来没变过，都是利用TD目标和用网络计算出的当前的价值张量进行均方误差传播下降。

在获得序列进行更新的时候，还可以采用“模拟目标网络”的方法，即得到了old_probs，即公式中的$\pi_{\theta_k}$，可以对网络更新多次。

至于PPO的主要流程，**和AC没有任何区别**，就是把AC算法中的**复杂梯度**换成当前这个复杂梯度即可。

## 算法9：DDPG算法

标签：无模型，离线策略，基于策略，用于连续动作空间，确定性策略（也正是因为用于连续动作空间，所以使得确定性策略能够保证可靠性）

对于动作空间连续的情况，利用DDPG可以制定一个确定性策略来指导智能体，并且可以利用离线策略，提高样本利用率。

于上述策略梯度下降类似，也可以得到一个优化目标$J(\theta)$对于参数$\theta$的梯度（证明略）：
$$
\nabla_\theta J(\theta) \approx\frac{1}{N}\sum^{N}_{i=1}\nabla_\theta Q_\omega(s_i,\mu_\theta(s_i))
$$
有了这个梯度，就可以和策略梯度一样进行传播下降操作更新策略了。但这个算法也有一些特点：

1. 确定性策略，探索较低，因此采取加入一个标准正态分布噪声进入动作采取过程。

2. 这里的目标不再是TD价值目标，价值网络Critic也不再是传入状态返回一个状态价值函数$V(s)$。现在的价值网络是传入状态和动作，返回的是动作价值函数$Q(s,a)$。所以这里的目标变成了以$Q$为主的TD动作目标，也即：
   $$
   y_i=r_i+\gamma Q_{\omega^-}(s_{i+1},\mu_{\theta^-}(s_{i+1}))
   $$
   用这个目标和当前价值网络计算出的$Q_{\omega^-}(s_i,a_i)$计算均方误差，用这个来反向传播更新价值网络。

3. 这里的价值网络和策略网络都采样目标网络方法，并且目标都使用目标网络进行计算得到。可以看出，目标网络适用于离线策略。（在线策略好像也可以？）

4. 目标网络的更新方式不再是采用周期更新法，而是采用软更新的方式。即目标网络的参数朝当前网络走一段距离（n维向量中的定比分点）。

**伪代码如下：**

![DDPG](RL_learn_png\DDPG.png)

可以看出DDPG算法和DQN算法的流程差不多：

1. 走一步，结果加入池子

2. 从池子里抽一点数据出来

3. 计算TD目标张量（集合），均方误差更新**价值网络**

4. 复杂梯度更新**策略网络**

5. **软更新**目标网络

   -> **以上步骤套入双循环**

区别就是双网络，以及软更新。还有就是，这里的价值网络要同时传入状态和动作，结构有点不一样。

## 算法10：SAC算法

标签：无模型，离线策略学习，随机性策略，动作连续（变成离散也可以）

由于DDPG算法有一些奇怪的缺点，所以SAC效果更好，主要采取了**最大熵**的方法。

**最大熵强化学习**的目的就是不但要最大化价值函数，而且还要让策略更加随机，也即把熵作为一个最大化项。这时状态价值函数可以被写为：
$$
V(s_t)=\mathbb E_{a_t \sim \pi}[Q(s_t,a_t)-\alpha\log\pi(a_t|s_t)]=\mathbb E_{a_t \sim \pi}[Q(s_t,a_t)]+\alpha H(\pi(\cdot|s_t))
$$

#### 价值网络的损失函数

价值网络的结构和DDPG是一样的（因为SAC本来就是DDPG plus），但这里有很多和DDPG不一样的地方。DDPG使用了4个网络，即AC+目标网络，而这里SAC使用了5个网络，4×C+1×A，有4个价值网络，分为两个Q网络和两个与之对应的目标网络。这里采用两个网络的原因是，当计算TD目标的时候，可以优先选择计算出的TD目标更小的那个网络来计算，这样可以一定程度上缓解Q值的过高估计。

可以得到价值网络的损失函数为：
$$
L_Q(\omega)=\mathbb E_{(s_t,a_t,r_t,s_{t+1})\sim R,a_{t+1}\sim\pi(\cdot|s_{t+1})}\bigg[\frac{1}{2}\Big(Q_\omega(s_t,a_t)-(r_t+\gamma(\min_{j=1,2}Q_{\omega_j^-}(s_{t+1},a_{t+1})-\alpha\log\pi(s_{t+1},a_{t+1})))\Big)^2\bigg]
$$
这里R是经验回放池，从中取出一部分数据，然后$a_{t+1}$是利用当前策略$\pi$选出来的结果（因为是离线策略，所以只能用这种方式来选择动作）。然后利用这个损失函数来对两个Q网络进行传播下降更新。目标网络还是采用软更新。

#### 策略网络的损失函数

经过奇怪的推导得到策略的损失函数为：
$$
L_\pi(\theta)=\mathbb E_{s_t\sim R,a_t\sim \pi_\theta}[\alpha\log(\pi_\theta(a_t|s_t))-Q_\omega(s_t,a_t)]
$$
不多废话，直接拿来更新即可。

#### 重参数化

在利用策略网络取参数的时候还有一点需要注意：如果是连续动作空间，为了保证随机性策略，需要获得均值和标准差，然后构造一个正态分布进行取值。但有一个问题：如果从构造的正态分布中取一个动作出来，那么这个动作对于均值和标准差是不可导的，也即没法反向传播用来更新网络。所以这里采用重参数化技巧：先从标准正态分布中取一个数，然后按照均值和标准差进行对应的映射得到需要的动作。这样就可以反向传播了。（实际上，这种取样方式直接一个rsample就可以搞定了）

外：取完了还没完，考虑到倒立摆模型，需要把取出来的东西对应到一个[-1,1]的区间内，所以需要把刚取出来的东西放入$\tanh$函数里面，然后把结果乘上动作的最大值才能得到最终需要的动作。

外+：得到了这个动作，它的概率怎么算？先是经过了一个高斯分布取样，然后又通过了$\tanh$函数处理，也即需要分析一个tanh-normal分布，得到概率密度函数再得出概率。下面记录一下我的分析过程：
$$
目标：得到\tanh-normal分布的概率密度\\
设\quad Y\sim N(\mu,\sigma),\quad X=\tanh ~Y\\
F_X(x)=P(X\le x)=P(\tanh~Y\le x)\\
\tanh~Y=\frac{1-e^{-Y}}{1+e^{-Y}}\\
可得：F_X(x)=P(Y\le\ln\frac{1+x}{1-x})=F_Y(\ln\frac{1+x}{1-x})\\
f_X(x)=f_Y(\ln\frac{1+x}{1-x})\frac{2}{1-x^2}\\
假设Y=y_0,x_0=\tanh~y_0\\
\ln f_X(x_0)=\ln f_Y(y_0)-\ln((1-x_0^2)/2)
$$
我感觉没啥问题，但是和网站给出的代码结果不一样:joy:。

#### 自动调整熵正则项

很nb的一个地方：自动根据情况修改熵的系数$\alpha$，损失函数为：
$$
L(\alpha)=\mathbb E_{s_t\sim R,a_t\sim\pi(\cdot|s_t)}[-\alpha\log(\pi(a_t|s_t))-\alpha\mathcal H_0]
$$
直接传播下降即可。

**伪代码如下：**（不知道这个训练轮数是什么东西）

![SAC](RL_learn_png\SAC.png)
